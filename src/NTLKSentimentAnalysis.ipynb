{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from random import shuffle\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics\n",
    "Following tutorial at https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/dylan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading sample data from NLTK library\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Compiling Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: ['PRESIDENT', 'HARRY', 'S', 'TRUMAN', 'S', 'ADDRESS', 'BEFORE', 'A', 'JOINT', 'SESSION']\n",
      "stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Load corpus\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()] # Filter only for words that comprise letters\n",
    "print(\"corpus:\", words[:10])\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print(\"stopwords:\", stopwords[:10])\n",
    "\n",
    "# Remove stopwords\n",
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenisation: ['This', 'is', 'a', 'simple', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation through NLTK\n",
    "text = \"This is a simple text.\"\n",
    "simple_words = nltk.word_tokenize(text)\n",
    "print(\"tokenisation:\", simple_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Frequency Distributions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency distribution: <FreqDist with 13810 samples and 180589 outcomes>\n",
      "most common words: [('must', 1568), ('people', 1291), ('world', 1128), ('year', 1097), ('America', 1076)]\n",
      "tabulated:\n",
      "   must  people   world    year America \n",
      "   1568    1291    1128    1097    1076 \n"
     ]
    }
   ],
   "source": [
    "# Build a frequency distribution\n",
    "# words: list[str] = nltk.word_tokenize(nltk.corpus.state_union.words())\n",
    "fd = nltk.FreqDist(words)\n",
    "print(\"frequency distribution:\", fd)\n",
    "print(\"most common words:\", fd.most_common(5))\n",
    "print(\"tabulated:\")\n",
    "fd.tabulate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of 'america': 0\n",
      "frequency of 'America': 1076\n",
      "frequency of 'AMERICA': 3\n",
      "frequency of 'america': 1079\n",
      "frequency of 'America': 0\n",
      "frequency of 'AMERICA': 0\n"
     ]
    }
   ],
   "source": [
    "# Frequency distributions can be queried by word for their frequency\n",
    "print(\"frequency of 'america':\", fd[\"america\"])\n",
    "print(\"frequency of 'America':\", fd[\"America\"])\n",
    "print(\"frequency of 'AMERICA':\", fd[\"AMERICA\"]) # Case sensitive\n",
    "\n",
    "# Normalise the frequency distribution to lowercase\n",
    "lower_fd = nltk.FreqDist(w.lower() for w in words)\n",
    "print(\"frequency of 'america':\", lower_fd[\"america\"])\n",
    "print(\"frequency of 'America':\", lower_fd[\"America\"])\n",
    "print(\"frequency of 'AMERICA':\", lower_fd[\"AMERICA\"]) # Case sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Extracting Concordance and Collocations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concordance**: Collection of word locations along with their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concordance (straight to console):\n",
      "Displaying 5 of 1079 matches:\n",
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n",
      "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
      " to make complete victory certain , America will never become a party to any pl\n",
      "nly in law and in justice . Here in America , we have labored long and hard to \n",
      "\n",
      "concordance_list (as list):\n",
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n",
      "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
      " to make complete victory certain , America will never become a party to any pl\n",
      "nly in law and in justice . Here in America , we have labored long and hard to \n"
     ]
    }
   ],
   "source": [
    "# Build new word list including stopwords\n",
    "# .concordance() uses nltk.Text class, also can be used with word list\n",
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "print(\"concordance (straight to console):\")\n",
    "text.concordance(\"america\", lines=5) # case insensitive\n",
    "\n",
    "# Use .concordance_list() to get the results as a list\n",
    "concordance_list = text.concordance_list(\"america\", lines=5)\n",
    "print(\"\\nconcordance_list (as list):\")\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample   This     is      a   text \n",
      "     2      1      1      1      1 \n"
     ]
    }
   ],
   "source": [
    "# ntlk.Text.vocab() returns a frequency distribution, similar to nltk.FreqDist()\n",
    "words : list[str] = nltk.word_tokenize(\"This is a sample sample text.\")\n",
    "text = nltk.Text(words)\n",
    "fd = text.vocab() # same as nltk.FreqDist(words)\n",
    "fd.tabulate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collocations**: Series of words that frequently appear together in a given text\n",
    "Bigrams, Trigrams, Quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bigrams:\n",
      "('of', 'the') ('in', 'the') ('to', 'the') \n",
      "         2599          1851          1143 \n",
      "\n",
      "trigrams:\n",
      "  ('the', 'United', 'States') ('the', 'American', 'people')        ('of', 'the', 'world') \n",
      "                          294                           185                           154 \n",
      "\n",
      "quadgrams:\n",
      "('of', 'the', 'United', 'States')         ('I', 'ask', 'you', 'to')   ('State', 'of', 'the', 'Union') \n",
      "                              110                                69                                58 \n"
     ]
    }
   ],
   "source": [
    "# Finding n-grams\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "\n",
    "print(\"\\nbigrams:\")\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "bigram_finder.ngram_fd.tabulate(3) # Using ngram_fd to obtain fd of n-grams\n",
    "\n",
    "print(\"\\ntrigrams:\")\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "trigram_finder.ngram_fd.tabulate(3)\n",
    "\n",
    "print(\"\\nquadgrams:\")\n",
    "quadgram_finder = nltk.collocations.QuadgramCollocationFinder.from_words(words)\n",
    "quadgram_finder.ngram_fd.tabulate(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Sentiment Analysis\n",
    "**VADER**: NLTK's built-in, pretrained sentiment analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Basic Usage of VADER_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Breakdown of VADER scores\n",
    "# neg + neu + pos = 1\n",
    "# -1 < compound < 0: negative\n",
    "# compound = 0: neutral\n",
    "# 0 < compound < 1: positive\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"This is a good example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: RT @faisalislam: Bingo: â€œ@AndrewSparrow: Only 6% of #bbcqt viewers said debate might change their mind - Clegg got most switchers - http//â€¦\n",
      "True: It's not like it hurt my feelings or anything right?? :))))))))))))\n",
      "False: 15 Days ago Danny took my wig and put it onto mark's head I want to go back there @thescript @TheScript_Danny :( â™¥ https//t.co/9ojA3FPxKF\n",
      "False: RT @joel_pearce: To all lefties voting Green, SNP, Lib Dem, Plaid etc etc: can you really deal with 5 more years of Tory gvt? I can't. #GE2â€¦\n",
      "False: The economic genius that is Margaret Farrier has made it to the Daily Record.\n",
      "\n",
      "http//t.co/7cvSJDKKib http//t.co/CHTsLlmsUi\n",
      "False: RT @mrmarksteel: Tomorrow Miliband will say 'let me tell you this, I'm not even voting for myself in case I do a deal with the SNP'.\n",
      "False: A don't get how someone can be Scottish and not support SNP\n",
      "True: ðŸŒžðŸŒžðŸŒž - :)))))))) stay perfect girly\n",
      "True: @DaveHShaw Hi Dave, please contact our in-App support chat so that we can ensure you get it on time! :)\n",
      "False: RT @OwenJones84: One week to kick the Tories out. If not, five more years of David Cameron, George Osborne, IDS, and Michael Gove. It's up â€¦\n"
     ]
    }
   ],
   "source": [
    "# Using VADER on Twitter samples\n",
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()] # raw tweets obtained as strings\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(f\"{is_positive(tweet)}: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using VADER on movie reviews\n",
    "pos_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "neg_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = pos_review_ids + neg_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite is_positive() to obtain movie review by ID then split into sentences before using VADER to rate\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    review = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    scores = [sia.polarity_scores(sentence)[\"compound\"] for sentence in sentences]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.00%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of VADER using is_positive() on movie reviews\n",
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "        if review_id in pos_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "        if review_id in neg_review_ids:\n",
    "            correct += 1\n",
    "accuracy = correct / len(all_review_ids)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Customising NLTK's Sentiment Analysis_\n",
    "**Features** as data properties used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting useful features\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()]) # Add names to stopwords\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"): # exclude nounds\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# pos_tag() returns a list of tuples, each containing a word and its part of speech\n",
    "pos_words = [word for word, tag in filter(\n",
    "    skip_unwanted, nltk.pos_tag(nltk.corpus.movie_reviews.words(categories = [\"pos\"])) \n",
    ")]\n",
    "\n",
    "neg_words = [word for word, tag in filter(\n",
    "    skip_unwanted, nltk.pos_tag(nltk.corpus.movie_reviews.words(categories = [\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_pos: {'exhilarating', 'sobbing', 'textured', 'deft', 'horned', 'weaves', 'fei', 'falter', 'propelled', 'unrestrained', 'belgian', 'masterfully', 'shrek', 'organizing', 'balancing', 'sparks', 'tibbs', 'spacey', 'pun', 'ulee', 'amistad', 'lumumba', 'curdled', 'elegantly', 'addresses', 'understatement', 'embeth', 'argento', 'tale', 'vividly', 'tibetan', 'uncut', 'soviet', 'hanks', 'benefit', 'seahaven', 'kimble', 'galactic', 'ghost', 'societal', 'valjean', 'uncompromising', 'attentive', 'apostle', 'sweetback', 'broadcast', 'forceful', 'unassuming', 'claiborne', 'profile', 'en', 'perceived', 'weir', 'melancholy', 'indistinguishable', 'safely', 'unquestionably', 'kudos', 'superficially', 'unnerving', 'conveys', 'powerfully', 'lovingly', 'vertical', 'maximus', 'taxing', 'notoriously', 'motta', 'methodical', 'flynt', 'fa', 'rico', 'unzipped', 'biased', 'donkey', 'niccol', 'matches', 'pink', 'supreme', 'jedi', 'legally', 'funnest', 'brisk', 'redefines', 'danish', 'radio', 'freed', 'shanghai', 'audacious', 'watson', 'trimmed', 'deftly', 'mulan', 'stendhal', 'farquaad', 'nello', 'narrates', 'monetary', 'criticized', 'ordell'}\n",
      "top_neg: {'squabble', 'stalks', 'heckerling', 'nitro', 'geronimo', 'brenner', 'jericho', 'droppingly', 'chi', 'unhealthy', 'undercut', 'warranted', 'amish', 'virus', 'busted', 'gordy', 'flipped', 'negated', 'interspersed', 'abysmal', 'audible', 'rabid', 'traced', 'nbsp', 'forgetful', 'tearing', 'chuckled', 'fetch', 'monumentally', 'embarassing', 'joely', 'goo', 'injury', 'mumbo', 'club', 'bean', 'popped', 'segal', 'crucible', 'modeled', 'terminal', 'topless', 'tectonic', 'performances', 'mandingo', 'consecutive', 'brazilian', 'wisecracking', 'verhoven', 'leaden', 'horrid', 'godzilla', 'disguise', 'grunting', 'ordering', 'tediously', 'precinct', 'degenerates', 'battlefield', 'wcw', 'supergirl', 'psychlo', 'weighed', 'stupidest', 'flubber', 'favors', 'stupidly', 'undeveloped', 'ego', 'enticing', 'unentertaining', 'rotating', 'lamest', 'digested', 'snipes', 'manchurian', 'leguizamo', 'glancing', 'schumacher', 'harlem', 'potty', 'artemus', 'iii', 'stinks', 'babe', 'sneering', 'incoherent', 'sphere', 'peripheral', 'deems', 'putrid', 'autistic', 'spawn', 'sans', 'rambo', 'plodding', 'mystery', 'pathetically', 'comment', 'pad'}\n"
     ]
    }
   ],
   "source": [
    "# Create fd for custom features\n",
    "pos_fd = nltk.FreqDist(pos_words)\n",
    "neg_fd = nltk.FreqDist(neg_words)\n",
    "\n",
    "common_set = set(pos_fd).intersection(neg_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del pos_fd[word]\n",
    "    del neg_fd[word]\n",
    "\n",
    "top_pos = {word for word, count in pos_fd.most_common(100)}\n",
    "top_neg = {word for word, count in neg_fd.most_common(100)}\n",
    "\n",
    "print(\"top_pos:\", top_pos)\n",
    "print(\"top_neg:\", top_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "positive bigrams:\n",
      "('special', 'effects')        ('new', 'york')     ('even', 'though') \n",
      "                   179                    131                    120 \n",
      "None\n",
      "\n",
      "negative bigrams:\n",
      "('special', 'effects')        ('new', 'york')     ('even', 'though') \n",
      "                   208                    118                    102 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find bigrams for features\n",
    "pos_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"]) if w.isalpha() and w not in unwanted\n",
    "])\n",
    "neg_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"]) if w.isalpha() and w not in unwanted\n",
    "])\n",
    "\n",
    "print(\"\\npositive bigrams:\")\n",
    "print(pos_bigram_finder.ngram_fd.tabulate(3)) \n",
    "\n",
    "print(\"\\nnegative bigrams:\")\n",
    "print(neg_bigram_finder.ngram_fd.tabulate(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training and Using a Classifer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from a given piece of data\n",
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    pos_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_pos:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(word)[\"compound\"])\n",
    "        pos_scores.append(sia.polarity_scores(word)[\"pos\"])\n",
    "\n",
    "    features[\"mean_compound\"] = mean(compound_scores)\n",
    "    features[\"mean_pos\"] = mean(pos_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features\n",
    "\n",
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in pos_review_ids\n",
    "]\n",
    "\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in neg_review_ids\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 4                 pos : neg    =      4.9 : 1.0\n",
      "               wordcount = 5                 pos : neg    =      4.3 : 1.0\n",
      "               wordcount = 2                 pos : neg    =      3.3 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.8 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.4 : 1.0\n",
      "           mean_compound = 0.0               pos : neg    =      1.0 : 1.0\n",
      "                mean_pos = 0.0               pos : neg    =      1.0 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6646666666666666"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a classifier\n",
    "train_count = len(features) // 4 # 25% of data for testing\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "nltk.classify.accuracy(classifier, features[train_count:]) # Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_compound': 0.0, 'mean_pos': 0.0, 'wordcount': 1}\n",
      "pos\n",
      "{'mean_compound': 0.0, 'mean_pos': 0.0, 'wordcount': 0}\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Test on new data\n",
    "new_pos_review = \"'Kingdom of the Planet of the Apes' emerges as a formidable successor to the legacy left by Caesar, \\\n",
    "    the venerable leader whose memory still looms large over this burgeoning saga. Situated approximately three centuries \\\n",
    "    after the tumultuous events of 'War for the Planet of the Apes,' this standalone entry serves as both an homage to its \\\n",
    "    predecessors and a bold new beginning for the franchise. In the absence of Caesar, Noa steps into the spotlight as the \\\n",
    "    central protagonist, carrying the weight of a new era on his shoulders. Noa's character is intricately crafted, embodying \\\n",
    "    a complex blend of reverence for Caesar's teachings and an audacious willingness to challenge them. This nuanced portrayal \\\n",
    "    of Noa is pivotal to the narrative, as his reinterpretation of Caesar's iconic mantraâ€”shifting from 'Apes together strong' \\\n",
    "    to the more inclusive 'No, together strong,' with humanity includedâ€”sets the philosophical and ideological groundwork for \\\n",
    "    the trilogy's future trajectory. The film delves deep into the fraught dynamics between humans and apes, refusing to simplify\\\n",
    "    the multifaceted layers of conflict and coexistence that define their interactions. This unflinching commitment to maintaining \\\n",
    "    the tension provides a rich soil for character growth and development, ensuring that the audience remains invested in the journey \\\n",
    "    of both species. Visually, 'Kingdom of the Planet of the Apes' is a testament to the continued technical prowess that the series \\\n",
    "    is known for. The seamless blend of motion-capture performances and cutting-edge visual effects not only brings the apes to life \\\n",
    "    with astonishing realism but also serves to elevate the emotional resonance of their struggles. The narrative is further enriched \\\n",
    "    by themes that explore leadership, identity, and the quest for a shared future, all of which resonate deeply within the current \\\n",
    "    social context. The film's ability to engage with these themes while delivering a gripping and entertaining story is a testament \\\n",
    "    to its thoughtful script and adept direction. In conclusion, 'Kingdom of the Planet of the Apes' stands as a compelling and \\\n",
    "    significant chapter in the storied franchise. It is a film that honors its roots while fearlessly forging ahead, offering \\\n",
    "    audiences a profound and thrilling experience that not only entertains but also invites reflection on our own world. With \\\n",
    "    Noa at the helm, the future of this new trilogy is ripe with potential, promising a cinematic journey that will continue \\\n",
    "    to captivate and challenge viewers for years to come.\"\n",
    "\n",
    "new_neg_review = \"I was really looking forward to watching this movie but was sadly disappointed. \\\n",
    "    The story line was boring and so were the characters.  I couldn't feel any emotional connection \\\n",
    "    with the characters unlike the previous movies.  The whole script was bland. I tried to convince \\\n",
    "    myself it was OK but realistically it's as entertaining as watching paint dry. Sadly for me however, \\\n",
    "    I was not entertained by the storyline or characters. Not many other people I spoke to liked it either.\"\n",
    "\n",
    "new_pos_features = extract_features(new_pos_review)\n",
    "new_neg_features = extract_features(new_neg_review)\n",
    "\n",
    "print(new_pos_features)\n",
    "print(classifier.classify(new_pos_features))\n",
    "\n",
    "print(new_neg_features)\n",
    "print(classifier.classify(new_neg_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audacious\n"
     ]
    }
   ],
   "source": [
    "for sentence in nltk.sent_tokenize(new_pos_review):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word.lower() in top_pos:\n",
    "            print(word)\n",
    "\n",
    "for sentence in nltk.sent_tokenize(new_neg_review):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word.lower() in top_neg:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Additional Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
